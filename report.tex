\documentclass[conference,a4paper]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Command to include a center figure
\newcommand{\centerfigure}[2]{
    \begin{figure}[htbp]
        \centering
        \caption{#2}
        \includegraphics[width=0.9\linewidth]{figures/#1.png}
        \label{fig:#1}
    \end{figure}
}
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{0pt}
\begin{document}

\title{AIML 425 Assignment 3 %\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{James Thompson}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{Victoria University of Wellington}\\
%City, Country \\
300680096}

% Figure example import part is the htbp
% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

\maketitle

\section{Introduction}

Autoencoders are a type of neural network uses for unsupervised learning of efficient latent space representations of data. An auto encoder consists of two main parts the encoder which compresses the input data into a lower dimensional space and the decoder which reconstructions the original data. Variational Autoencoder are improved Autoencoders that change the output of the encoder to be a distribution over the latent space rather than a fixed vector and uses a ELBO loss function. I will explore both the generative capabilities of regular Autoencoders and Variational Autoencoders as well as the meaning of the latent space and how much information is retained in the latent space. I will train both models on a generated dataset of shapes of different size and locations on a 28x28 canvas. Both models achieve good reconstruction of the input data, however only the VAE is able to generate reasonable new images by sampling the latent space.

\section{Theory}

Autoencoder and their variations are a type of neural networks arhcitecture that are used for unsupervised learning of an efficient representation of the input data. Basic autoencoders use a encoder-decoder architecture to compress and then reconstruct the input data. Variational Autoencoders (VAEs) extend this architecture by introducing a probabilistic approach to the encoding process, allowing for the latent space to be a known distribution. This section provides a description of both model types as well as a discussion of the latent space.

\subsection{Autoencoders}

TODO: add mathematical/probablistic description of model

An Auto encoder has three important parts the encoder $f_{enc}$, the decoder $f_{dec}$ and the latent layer $z$. The latent layer is typically of a much lower dimension (2D-32D) than the input data. The encoder and decoder are typically implemented as neural networks with learnable parameters $\phi$ and $\theta$. Furthermore the encoder and decoder are generally the inverse of each other.

\begin{align}
z &= f_{enc}(x; \phi)\\
\hat{x} &= f_{dec}(z; \theta)
\end{align}
The input data $x$ goes through $f_{enc}$ to $z$ then through $f_{dec}$ to produce the reconstruction $\hat{x}$. This model is then trained based on the reconstruction loss between the input and output data.

\begin{align}
\mathcal{L}_{AE} &= \| x - \hat{x} \|^2\\
\end{align}

Training with this loss function means that the model will learn a compressed representation of the data. However there is no reason that the distribution of the latent space will be smooth or continuous (i.e that all squares will be near each other in the latent space). This means that the generative capabilities of the model will be limited. Only certain part of the latent space will correspond to valid images, without access to the encoder and training data we have no way of knowing which parts of the latent space are valid. This means that sampling the latent space to generate new images is likely to produce random noise rather than valid images.

\subsection{Variational Autoencoders}

Variational Autoencoders (VAEs) are a type of generative model that builds upon the architecture of traditional autoencoders by introducing a probabilistic approach to the encoding process. In a VAE, the encoder does not produce a single point in the latent space but rather a distribution over the latent space, typically modeled as a Gaussian distribution. The goal is to learn the parameters of the generative model $p_{\theta}(x|z)$ which would allow us to generate new data points by sampling from the latent space.

\begin{align}
q(z|x) &= \mathcal{N}(z; \mu(x),\text{diag}(\sigma^2(x)))\\
\mu(x), \sigma(x) &= f_{enc}(x; \phi) \text{ where } \mu \text{ is mean and } \sigma \text{ is variance} \notag \\
z &\sim q(z|x)\\
\hat{x} &= f_{dec}(z; \theta)
\end{align}

This changes the latent space so that it comes from a normal distribution. Therefore the latent space is continuous, smooth and has a known distribution. The latent space will now be valid everywhere in the space, meaning that we can sample from the latent space to generate new images.

Furthermore, VAEs are trained using a loss function that combines the reconstruction loss (similar to traditional autoencoders) with a regularization term that encourages the learned latent distributions to be close to a prior distribution (typically a standard normal distribution). This regularization is achieved using the Kullback-Leibler (KL) divergence, which measures the difference between the learned distribution and the prior distribution. The overall loss function for a VAE can be expressed as:

\begin{align}
\mathcal{L}_{VAE} &= \mathcal{L}_{reconstruction} + \beta \mathcal{L}_{KL}\\
\text{Using}&\text{ MSE for reconstruction loss and Gaussian prior:} \notag\\
&= \mathbb{E}_{x \sim p_{data}(x)}\left[\log p_{\theta}(x|z)\right] - \beta D(q(z|x)||p(z)) \notag\\
&= \| x - \hat{x} \|^2 + \beta \frac{1}{2} \sum_{i=1}^{d} \left( \mu_i^2 + \sigma_i^2 - \log(\sigma_i^2) - 1 \right)
\end{align}

This loss function is derived using the Jensen's inequality to maximize the evidence lower bound (ELBO) of the data likelihood, which will also maximize the likelihood of $\theta$. The reconstruction loss ensures that the decoded outputs are similar to the inputs, while the KL divergence term regularizes the latent space to follow the prior distribution. The hyperparameter $\beta$ controls the trade-off between these two objectives.

For back propagation through the stochastic sampling process, VAEs employ the reparameterization trick. This involves expressing the latent variable $z$ as a deterministic function of the encoder outputs and a random noise variable $\epsilon$ sampled from a standard normal distribution:
\begin{align}
z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
\end{align}

Where $\epsilon$ is sampled for the minibatch and assumed to be constant for the back-propagation step. This allows gradients to be computed with respect to the encoder parameters, enabling effective training of the VAE using standard backpropagation techniques.


\subsection{Latent space information}

\subsubsection{Information rate}

When we compress data, we inevitably lose some information. The amount of information retained in the compressed representation can be quantified using the concept of information rate. Assuming that we add noise to the latent space, $z = y + \epsilon$ where $\epsilon \sim \mathcal{N}(0, \sigma^2)$, the information rate can be calculated as:

\begin{align}
R = \sum_i \frac{1}{2} \log_2 \left(1 + \frac{\sigma_{i,Z}^2}{\sigma_{i, \epsilon}^2}\right)
\end{align}

Where $\sigma_{i,Z}^2$ is the variance of the $i$-th dimension of the latent space and $\sigma_{i, \epsilon}^2$ is the variance of the noise added to that dimension.

This means that for a given noise level, we can estimate the amount of information that can be retained in the latent space by sampling the encoder outputs and measuring the variance of each dimension.

\subsubsection{Information meaning}

The information that the latent space retain can be interpreted in multiple ways. One way is that each dimension of the latent space could represent a specific feature of the input data. For example in my data generation process I generate each shape with only 4 numbers (x position, y position, size, shape). Therefore one could imagine a 4D latent space that perfectly encodes these 4 features. In practice this is unlikely to happen, but we can still interpret the latent space as a compressed representation of the input data.

\section{Experiments}

\subsection{Dataset}

The dataset used for training both models is a generated dataset of simple shapes (circles, squares, triangles) on a 28x28 canvas. Each shape is generated with random size and position. The shape may go up to the bounds but will never go out it and be clipped. There are 8 sizes ($[7,14]$) which means that there are a total 

\subsection{Variational autoencoder}

\subsubsection{setup}

\subsubsection{generative performance}

\subsection{Autoencoder}

\subsubsection{setup}

\subsubsection{Inforamtion rate}

\subsubsection{Information meaning}

\subsection{Autoencoder vs Variational autoencoder}


\section{Conclusion}

\newpage
\section*{Statement}

TODO: add colab link
The code and report of the Assignment was solely completed by myself (Thompson James). The complete source code can be found here \url{https://gitea.james-server.duckdns.org/james/AIML425\_assignment\_3}, with a link to a colab notebook found here: \url{TODO: add colab link}. A complete run through of the notebook takes about an hour to complete on a university lab machine.

I completed my work using the following tools:
\begin{itemize}
    \item \textbf{Jupyter Notebook \cite{Kluyver2016jupyter} and JupyterText \cite{woutsMwoutsJupytext2025}:} For interactive development and hosting.
    \item \textbf{\LaTeX}: For writing the report.
    \item \textbf{VSCode \cite{MicrosoftVscode2025}:} As IDE.
    \item \textbf{JAX \cite{jax2018github} and Flax \cite{flax2020github}:} For implementing the neural network and training logic.
    \item \textbf{Matplotlib\cite{Hunter:2007} and Pandas\cite{thepandasdevelopmentteamPandasdevPandasPandas}:} For data visualization and management.
\end{itemize}


\bibliographystyle{IEEEtran}
\bibliography{references}


\end{document}
