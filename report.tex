\documentclass[conference,a4paper]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Command to include a center figure
\newcommand{\centerfigure}[2]{
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.9\linewidth]{figures/#1.png}
        \caption{#2}
        \label{fig:#1}
    \end{figure}
}

\newcommand{\centertable}[2]{
    \begin{table}[htbp]
        \centering
        \caption{#2}
        \input{figures/#1.tex}
        \label{tab:#1}
    \end{table}
}

\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{0pt}
\begin{document}

\title{AIML 425 Assignment 3 %\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{James Thompson}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{Victoria University of Wellington}\\
%City, Country \\
300680096}

% Figure example import part is the htbp
% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

\maketitle

\section{Introduction}
The report documents the implementation and results of Assignment 2 problem 2. I implement a Variational Autoencoder (VAE) and a basic Autoencoder (AE), compare their generative performance, and analyze the AEâ€™s latent space information.

\section{Theory}

Autoencoders and their variations are a type of neural networks architecture that are used for unsupervised learning of an efficient representation of the input data. This section will introduce auto encoders, variational auto encoders as well as some concepts for understanding the latent space and generative performance.

\subsection{Autoencoders (AE)}

An Autoencoder \cite{hintonReducingDimensionalityData2006} has three important parts the encoder $f_{enc}$, the decoder $f_{dec}$ and the latent layer $z$ (typically with lower dimension than input). The encoder and decoder are neural networks with learnable parameters $\phi$ and $\theta$.

\begin{align}
z &= f_{enc}(x; \phi) \quad \hat{x} = f_{dec}(z; \theta)
\end{align}
The weights are trained based on the reconstruction loss between the input and output data.

\begin{align}
\mathcal{L}_{AE} &= \| x - \hat{x} \|^2 + \beta MMD(z, \mathcal{N}(0, I))
\end{align}

Training with the with $\beta=0$ means that the model will learn a compressed representation of the data. However there is no reason that the distribution of the latent space will be smooth or continuous (i.e that all squares will be near each other in the latent space). This means that the generative capabilities of the model will be limited. Only certain part of the latent space will correspond to valid images. To fix this we can add a regularizer like Maximum Mean discrepancy \cite{grettonKernelMethodTwoSample2008} that will push the latent space to Gaussian distributions, this is what the second loss function is.

\subsection{Variational Autoencoders (VAE)}

In a VAE \cite{kingmaAutoEncodingVariationalBayes2022}, the encoder does not produce a single point in the latent space but rather parameters for a Gaussian distribution. The goal is to learn the generative model $p_\theta(x)$, which we do by learning $p_\theta(x|z)$ where $z$ is from a known distribution. To do this we also  $q_\phi(z|x)$ that gives us the latent distribution given the input.

\begin{align}
q_\phi(z|x) &= \mathcal{N}(z; \mu(x),\text{diag}(\sigma^2(x)))\\
\text{where } \mu(x), \sigma(x) &= f_{enc}(x; \phi) \text{ and } z \sim q_\phi(z|x) \notag\\
\hat{x} &= f_{dec}(z; \theta)
\end{align}

Like with traditional autoencoders, VAEs are trained to minimize a loss function that consists of two main components: the reconstruction loss and the regularization loss. The reconstruction error can be measured with Binary Cross Entropy and regularization with the Kullback-Leibler (KL) \cite{kullbackInformationSufficiency1951} divergence between the learned latent distribution and a prior distribution.

\begin{align}
\mathcal{L}_{VAE} &= \mathcal{L}_{reconstruction} + \beta \mathcal{L}_{KL}\\
\text{Using}&\text{ BCE for reconstruction loss and Gaussian prior:} \notag\\
&= -\sum_{i=1}^{d} \left[ x_i \log(1+\exp(-\hat{x}_i)) + (1 - x_i)  \log(1+\exp(\hat{x}_i)) \right]\notag \\ &+ \beta \frac{1}{2} \sum_{i=1}^{d} \left( \mu_i^2 + \sigma_i^2 - \log(\sigma_i^2) - 1 \right)
\end{align}
\noindent Where $d$ is the dimensionality of the input data and $\beta$ is a hyperparameter. This loss function is derived using the Jensen's inequality to maximize the log likelihood of $p_\theta(x)$ \cite{kingmaAutoEncodingVariationalBayes2022}. To allow back propagation through the stochastic sampling process, VAEs employ the reparameterization trick.

\subsection{Latent space information}

\subsubsection{Information rate}
Information rate is the amount of information that can pass through the latent layer. Assuming that we add noise to the latent space, $z = y + \epsilon$ where $\epsilon \sim \mathcal{N}(0, \sigma^2)$, the information rate can be calculated as:

\begin{align}
R = \sum_i \frac{1}{2} \log_2 \left(1 + \frac{\sigma_{i,Z}^2}{\sigma_{i, \epsilon}^2}\right)
\label{eq:info_rate}
\end{align}

\noindent Where $\sigma_{i,Z}^2$ is the variance of the $i$-th dimension of the latent space and $\sigma_{i, \epsilon}^2$ is the variance of the noise added to that dimension.

\subsubsection{Information meaning}

The information that the latent space retains can be interpreted in multiple ways. One way is that each dimension of the latent space could represent a specific feature of the input data. We can test for this using correlation between known features and latent dimensions. Or we could test it by varying individual dimension and seeing how the reconstruction changes. In practice each dimension is unlikely to represent a single feature, but rather a combination of features.


\subsection{Generative performance}
To understand the generative performance of the models we can use the set of valid images to compare against. This allows us to ask the question; how many of the valid images can the model generate (coverage), how close are the generated images to the nearest valid image (nearest neighbor performance) and how similar is the distribution of generated images to the distribution of valid images (KL divergence). These metrics are defined as follows:

\begin{align*}
\text{coverage} &= \frac{|\{x \in \text{images} \mid \min_{\hat{x} \in \text{generated}} d(x, \hat{x}) < \text{thres}\}|}{|\text{images}|}\\
\text{nearest neighbor} &= \frac{1}{|\text{images}|} \sum_{x_i \in \text{images}} \min_{\hat{x}_j \in \text{generated}} d(x_i, \hat{x}_j)\\
\text{KL divergence} &\approx \sum_{x_i \in \text{images}} p_{data}(x_i) \log \frac{p_{data}(x_i)}{p_{model}(x_i)}\\
&= \sum_{x_i \in \text{images}} \frac{1}{|images|} \log \frac{1}{q_{counts}(x_i)}\\
\end{align*}
\noindent Where $d(x_i, \hat{x}_j)$ is the distance between two images, and the $\text{thres}$ is a hyperparameter, and $q_{counts}(x_i)$ is the number of times the model has generated an image close enough to $x_i$ (within the threshold).

\section{Experiments}

\subsection{Dataset}

The dataset used for training both models is a generated dataset of simple shapes (circles, squares, triangles) on a 28x28 canvas. There are 8 sizes($[7,14]$) which means that there are a total 8002 unique images. The images are white on black. I used 90\% for training and 10\% for validation. The validation set is simply used to check for overfitting, all other metrics use the full set of images.

\subsection{Variational autoencoder}
    
\subsubsection{setup}

The VAE model is two fully connected neural networks. The encoder outputs 2 values (mean and variance) while the decoder outputs a single scalar. The simplicity of the network was chosen due to the relatively simple images. The loss function used was the ELBO loss function with a $\beta$ of 0.65. A complete list of hyperparameters is shown in table \ref{tab:hyperparams}. The fine tuning of parameters was done through trial and error using the loss curve and generative performance metrics as a guide.

\subsubsection{generative performance}

I use euclidean as the distance metric and the threshold of 3 in the generative metrics. I choose this by looking at sample images and aligning the cutoff and distance metric with my intuition. The results of the generative performance can be found in \ref{tab:ae-vs-vae} along with example generations in figure \ref{fig:vae-generated}. The model is found to be able to generate just over half of the input images and the nearest neighbor distance is about 2.4 pixels.

\subsection{Autoencoder}

\subsubsection{setup}
The autoencoder was built off the same architecture as the VAE, except that the encoder outputs a single value and the loss function is the modified MSE loss function. A complete list of hyperparameters is shown in table \ref{tab:hyperparams}. It took more fine-tuning to get any generative performance out of the model.

\subsubsection{Information rate}

The information rate of the latent space was calculated by getting the encoder outputs from all of the possible images. Then using equation \ref{eq:info_rate} to calculate the information rate. The estimated information was 15.7 bits. This means it uses just less information than a single half float value. About 14.2 bits of information are required to encode the 4 parameters of the shapes (x, y, size, shape) so the model is using about 1.9 bits of extra information to encode the data.

\subsubsection{Information meaning}

There are two methods that I will use to understand the latent space. The first is to check for correlation between known features (x position, y position, size, shape) and the latent dimensions. The second is to change the latent dimensions and see how the reconstruction changes, I do by both zeroing out and conducting traversals.

Figure \ref{fig:ae-latent-corelation} shows that there is some correlation between some latent dimensions. Particularly the size has some correlations around 0.6, other latent have some weak correlations ~0.3 but nothing strong. Furthermore looking at the distribution of figure \ref{fig:ae-latent-by-shape} we can see that there is no strong correlation between shape and any latent dimension.

Looking at figure \ref{fig:ae-traversal-2} shows that some dimensions have a large effect. For example latent dimension 8 can be seen to reduce the size of the shape as it increase. We can see in most of the dimensions that if you move too far away from the mean (0) then the generations become meaningless, this shows how only a small part of the latent space is understood by the AE model.

\subsection{Autoencoder vs Variational autoencoder}

Using the metrics discussed above we can measure the generative performance of both models quantitatively.

\centertable{ae-vs-vae}{Comparison of performance between the VAE and AE models.}

We can see in \ref{tab:ae-vs-vae} that the VAE model outperforms the AE models on almost all the metrics. It has noticeably lower average nearest neighbor and higher coverage. Both models however still fail to capture the full distribution of the data missing about 40\% of the valid images.


\section{Conclusion}

I trained two different models a VAE and an AE on a simple dataset of shapes. Both models were able to effectively compress the data. The VAE model had better generative performance. An investigation of the latent space of the AE model showed some latent dimensions have correlated effects to known features. I would expect that more complex models and hyperparameter search could yield better results.

\newpage
\section*{Statement}

The code and report of the Assignment was solely completed by myself (Thompson James). The complete source code can be found here \url{https://gitea.james-server.duckdns.org/james/AIML425\_assignment\_3}, with a link to a colab notebook found here: \url{https://colab.research.google.com/github/1jamesthompson1/AIML425_assignment_3/blob/main/main.ipynb}. A complete run through of the notebook takes about 10 minutes on a GPU enabled machine.

I have kept the appendix as concise as possible, however the code is setup to produce many more figures and tables that can be used to understand the models. Most of these figures have been generated and stored in the `figures` folder.

I completed my work using the following tools:
\begin{itemize}
    \item \textbf{Jupyter Notebook \cite{Kluyver2016jupyter} and JupyterText \cite{woutsMwoutsJupytext2025}:} For interactive development and hosting.
    \item \textbf{\LaTeX}: For writing the report.
    \item \textbf{VSCode \cite{MicrosoftVscode2025}:} As IDE, with Copilot to help with plotting and debugging.
    \item \textbf{JAX \cite{jax2018github} and Flax \cite{flax2020github}:} For implementing the neural network and training logic.
    \item \textbf{Matplotlib\cite{Hunter:2007} and Pandas\cite{thepandasdevelopmentteamPandasdevPandasPandas}:} For data visualization and management.
\end{itemize}


\bibliographystyle{IEEEtran}
\bibliography{references}

\appendix

\section{Models}

\begin{table}[htbp]
\caption{Hyperparameters used for training the VAE and AE models.}
\label{tab:hyperparams}
\centering
\begin{tabular}{l|c|c}
\toprule
\textbf{Hyperparameter} & \textbf{VAE} & \textbf{AE} \\
\midrule
Latent dimension & 32 & 10 \\
Encoder architecture & [1000, 1000, 1000, 500] & [1000, 1000, 500] \\
Decoder architecture & [500, 1000, 1000, 1000] & [500, 1000, 1000] \\
Optimizer & Adam(lr=0.001) & Adam(lr=0.001) \\
Minibatch size & 512 & 64 \\
Number of epochs & 1000 & 500 \\
Regularization weight & 0.65 & 0.1 \\
Regularization parameters & NA &  sigma=$[0.5,1,3,5]$\\
Dropout & Yes (p=0.1) & Yes (p=0.1) \\
Layer norm & No & No \\
Latent space noise & No & Yes ($\sigma=0.2$) \\
\bottomrule
\end{tabular}
\end{table}

\centerfigure{vae-generated}{Samples generated from the trained VAE model. The model is able to generate a variety of shapes, sizes, and positions.}

\section{Understanding latent space}

\centerfigure{ae-latent-by-shape}{Distribution of latent dimensions grouped by shape for the AE model. No strong correlation between shape and any latent dimension is visible.}

\centerfigure{ae-latent-corelation}{Correlation between known features and latent dimensions for the AE model. Some strong correlation is visible, particularly for y position.}

\centerfigure{ae-traversal-2}{Latent space traversals for the AE model. Each row shows the effect of varying one latent dimension from -3 to 3 while keeping the others constant.}

\end{document}
